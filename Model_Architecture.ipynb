{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89850,"databundleVersionId":11256103,"sourceType":"competition"},{"sourceId":228781,"sourceType":"modelInstanceVersion","modelInstanceId":195042,"modelId":216938}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThe purpose of this notebook is to show how the provided model is built","metadata":{}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport timm \nimport torch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nimport os\nimport torchvision.transforms as T\nfrom torch.amp import autocast\nfrom matplotlib import pyplot as plt\nfrom kornia import tensor_to_image\nfrom kornia.contrib import extract_tensor_patches, compute_padding\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:54:18.103669Z","iopub.execute_input":"2025-10-17T16:54:18.103955Z","iopub.status.idle":"2025-10-17T16:54:28.893038Z","shell.execute_reply.started":"2025-10-17T16:54:18.103931Z","shell.execute_reply":"2025-10-17T16:54:28.892099Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_species_ids = pd.read_csv('/kaggle/input/plantclef-2025/species_ids.csv')\n\ndf_metadata = pd.read_csv('/kaggle/input/plantclef-2025/PlantCLEF2024_single_plant_training_metadata.csv', sep=';', dtype={'partner': str})\nclass_map = df_species_ids['species_id'].to_dict() # dictionary to map the species model Id with the species Id\n\ndf_metadata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:55:42.163211Z","iopub.execute_input":"2025-10-17T16:55:42.163800Z","iopub.status.idle":"2025-10-17T16:56:02.067961Z","shell.execute_reply.started":"2025-10-17T16:55:42.163770Z","shell.execute_reply":"2025-10-17T16:56:02.067132Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                     image_name organ  species_id      obs_id  \\\n0  59feabe1c98f06e7f819f73c8246bd8f1a89556b.jpg  leaf     1396710  1008726402   \n1  dc273995a89827437d447f29a52ccac86f65476e.jpg  leaf     1396710  1008724195   \n2  416235e7023a4bd1513edf036b6097efc693a304.jpg  leaf     1396710  1008721908   \n3  cbd18fade82c46a5c725f1f3d982174895158afc.jpg  leaf     1396710  1008699177   \n4  f82c8c6d570287ebed8407cefcfcb2a51eaaf56e.jpg  leaf     1396710  1008683100   \n\n    license partner          author  altitude   latitude  longitude  \\\n0  cc-by-sa     NaN   Gulyás Bálint  205.9261  47.592160  19.362895   \n1  cc-by-sa     NaN    vadim sigaud  323.7520  47.906703   7.201746   \n2  cc-by-sa     NaN     fil escande  101.3160  48.826774   2.352774   \n3  cc-by-sa     NaN  Desiree Verver    5.1070  52.190427   6.009677   \n4  cc-by-sa     NaN      branebrane  165.3390  45.794739  15.965862   \n\n   gbif_species_id           species  genus    family   dataset publisher  \\\n0        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n1        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n2        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n3        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n4        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n\n                                          references  \\\n0  https://identify.plantnet.org/fr/k-southwester...   \n1  https://identify.plantnet.org/fr/k-southwester...   \n2  https://identify.plantnet.org/fr/k-southwester...   \n3  https://identify.plantnet.org/fr/k-southwester...   \n4  https://identify.plantnet.org/fr/k-southwester...   \n\n                                                 url learn_tag  \\\n0  https://bs.plantnet.org/image/o/59feabe1c98f06...     train   \n1  https://bs.plantnet.org/image/o/dc273995a89827...     train   \n2  https://bs.plantnet.org/image/o/416235e7023a4b...     train   \n3  https://bs.plantnet.org/image/o/cbd18fade82c46...     train   \n4  https://bs.plantnet.org/image/o/f82c8c6d570287...     train   \n\n                                    image_backup_url  \n0  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n1  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n2  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n3  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n4  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>organ</th>\n      <th>species_id</th>\n      <th>obs_id</th>\n      <th>license</th>\n      <th>partner</th>\n      <th>author</th>\n      <th>altitude</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>gbif_species_id</th>\n      <th>species</th>\n      <th>genus</th>\n      <th>family</th>\n      <th>dataset</th>\n      <th>publisher</th>\n      <th>references</th>\n      <th>url</th>\n      <th>learn_tag</th>\n      <th>image_backup_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59feabe1c98f06e7f819f73c8246bd8f1a89556b.jpg</td>\n      <td>leaf</td>\n      <td>1396710</td>\n      <td>1008726402</td>\n      <td>cc-by-sa</td>\n      <td>NaN</td>\n      <td>Gulyás Bálint</td>\n      <td>205.9261</td>\n      <td>47.592160</td>\n      <td>19.362895</td>\n      <td>5284517.0</td>\n      <td>Taxus baccata L.</td>\n      <td>Taxus</td>\n      <td>Taxaceae</td>\n      <td>plantnet</td>\n      <td>plantnet</td>\n      <td>https://identify.plantnet.org/fr/k-southwester...</td>\n      <td>https://bs.plantnet.org/image/o/59feabe1c98f06...</td>\n      <td>train</td>\n      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dc273995a89827437d447f29a52ccac86f65476e.jpg</td>\n      <td>leaf</td>\n      <td>1396710</td>\n      <td>1008724195</td>\n      <td>cc-by-sa</td>\n      <td>NaN</td>\n      <td>vadim sigaud</td>\n      <td>323.7520</td>\n      <td>47.906703</td>\n      <td>7.201746</td>\n      <td>5284517.0</td>\n      <td>Taxus baccata L.</td>\n      <td>Taxus</td>\n      <td>Taxaceae</td>\n      <td>plantnet</td>\n      <td>plantnet</td>\n      <td>https://identify.plantnet.org/fr/k-southwester...</td>\n      <td>https://bs.plantnet.org/image/o/dc273995a89827...</td>\n      <td>train</td>\n      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>416235e7023a4bd1513edf036b6097efc693a304.jpg</td>\n      <td>leaf</td>\n      <td>1396710</td>\n      <td>1008721908</td>\n      <td>cc-by-sa</td>\n      <td>NaN</td>\n      <td>fil escande</td>\n      <td>101.3160</td>\n      <td>48.826774</td>\n      <td>2.352774</td>\n      <td>5284517.0</td>\n      <td>Taxus baccata L.</td>\n      <td>Taxus</td>\n      <td>Taxaceae</td>\n      <td>plantnet</td>\n      <td>plantnet</td>\n      <td>https://identify.plantnet.org/fr/k-southwester...</td>\n      <td>https://bs.plantnet.org/image/o/416235e7023a4b...</td>\n      <td>train</td>\n      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cbd18fade82c46a5c725f1f3d982174895158afc.jpg</td>\n      <td>leaf</td>\n      <td>1396710</td>\n      <td>1008699177</td>\n      <td>cc-by-sa</td>\n      <td>NaN</td>\n      <td>Desiree Verver</td>\n      <td>5.1070</td>\n      <td>52.190427</td>\n      <td>6.009677</td>\n      <td>5284517.0</td>\n      <td>Taxus baccata L.</td>\n      <td>Taxus</td>\n      <td>Taxaceae</td>\n      <td>plantnet</td>\n      <td>plantnet</td>\n      <td>https://identify.plantnet.org/fr/k-southwester...</td>\n      <td>https://bs.plantnet.org/image/o/cbd18fade82c46...</td>\n      <td>train</td>\n      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f82c8c6d570287ebed8407cefcfcb2a51eaaf56e.jpg</td>\n      <td>leaf</td>\n      <td>1396710</td>\n      <td>1008683100</td>\n      <td>cc-by-sa</td>\n      <td>NaN</td>\n      <td>branebrane</td>\n      <td>165.3390</td>\n      <td>45.794739</td>\n      <td>15.965862</td>\n      <td>5284517.0</td>\n      <td>Taxus baccata L.</td>\n      <td>Taxus</td>\n      <td>Taxaceae</td>\n      <td>plantnet</td>\n      <td>plantnet</td>\n      <td>https://identify.plantnet.org/fr/k-southwester...</td>\n      <td>https://bs.plantnet.org/image/o/f82c8c6d570287...</td>\n      <td>train</td>\n      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Auto-detect device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Create model\nmodel = timm.create_model('vit_base_patch14_reg4_dinov2.lvd142m',\n                          pretrained=False,\n                          num_classes=len(df_species_ids),\n                          checkpoint_path='/kaggle/input/dinov2_patch14_reg4_onlyclassifier_then_all/pytorch/default/3/model_best.pth.tar')\n\n# Move model to device\nmodel = model.to(device)\n\n# Set model to evaluation mode\nmodel = model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:57:18.464733Z","iopub.execute_input":"2025-10-17T16:57:18.465059Z","iopub.status.idle":"2025-10-17T16:57:31.367270Z","shell.execute_reply.started":"2025-10-17T16:57:18.465034Z","shell.execute_reply":"2025-10-17T16:57:31.366400Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install torchviz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:58:44.714158Z","iopub.execute_input":"2025-10-17T16:58:44.714441Z","iopub.status.idle":"2025-10-17T16:58:49.094374Z","shell.execute_reply.started":"2025-10-17T16:58:44.714420Z","shell.execute_reply":"2025-10-17T16:58:49.093490Z"}},"outputs":[{"name":"stdout","text":"Collecting torchviz\n  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.5.1+cu121)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (3.0.2)\nDownloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:58:52.377353Z","iopub.execute_input":"2025-10-17T16:58:52.377703Z","iopub.status.idle":"2025-10-17T16:58:52.383952Z","shell.execute_reply.started":"2025-10-17T16:58:52.377671Z","shell.execute_reply":"2025-10-17T16:58:52.382930Z"}},"outputs":[{"name":"stdout","text":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=768, out_features=7806, bias=True)\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(model.__class__.__name__)\nprint(model.default_cfg)\nprint(model.pretrained_cfg)\nprint(model.keys())  # only for some timm models\n\nfor name, module in model.named_children():\n    print(name, \":\", module.__class__.__name__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:58:56.477163Z","iopub.execute_input":"2025-10-17T16:58:56.477452Z","iopub.status.idle":"2025-10-17T16:58:56.635954Z","shell.execute_reply.started":"2025-10-17T16:58:56.477431Z","shell.execute_reply":"2025-10-17T16:58:56.634738Z"}},"outputs":[{"name":"stdout","text":"VisionTransformer\n{'url': 'https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth', 'hf_hub_id': 'timm/vit_base_patch14_reg4_dinov2.lvd142m', 'architecture': 'vit_base_patch14_reg4_dinov2', 'tag': 'lvd142m', 'custom_load': False, 'input_size': (3, 518, 518), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 1.0, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 0, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'license': 'apache-2.0'}\n{'url': 'https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth', 'hf_hub_id': 'timm/vit_base_patch14_reg4_dinov2.lvd142m', 'architecture': 'vit_base_patch14_reg4_dinov2', 'tag': 'lvd142m', 'custom_load': False, 'input_size': (3, 518, 518), 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 1.0, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'num_classes': 0, 'pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'license': 'apache-2.0'}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-4d3587cb3dc0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# only for some timm models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'keys'"],"ename":"AttributeError","evalue":"'VisionTransformer' object has no attribute 'keys'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total params: {total_params:,}\")\nprint(f\"Trainable params: {trainable_params:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:58:59.141499Z","iopub.execute_input":"2025-10-17T16:58:59.141857Z","iopub.status.idle":"2025-10-17T16:58:59.148823Z","shell.execute_reply.started":"2025-10-17T16:58:59.141829Z","shell.execute_reply":"2025-10-17T16:58:59.147888Z"}},"outputs":[{"name":"stdout","text":"Total params: 92,584,830\nTrainable params: 92,584,830\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"block = model.blocks[0]\nx = torch.randn(1, 197, 768).to(device)  # sequence of patch embeddings\ny = block(x)\n\ndot = make_dot(y, params=dict(block.named_parameters()))\ndot.render(\"vit_block\", format=\"pdf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:00:02.529464Z","iopub.execute_input":"2025-10-17T17:00:02.529828Z","iopub.status.idle":"2025-10-17T17:00:02.568878Z","shell.execute_reply.started":"2025-10-17T17:00:02.529801Z","shell.execute_reply":"2025-10-17T17:00:02.568140Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'vit_block.pdf'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from torchviz import make_dot\n\ndot = make_dot(y, params=dict(model.named_parameters()))\ndot.render(\"vit_model_graph\", format=\"pdf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:00:05.043362Z","iopub.execute_input":"2025-10-17T17:00:05.043741Z","iopub.status.idle":"2025-10-17T17:00:05.079590Z","shell.execute_reply.started":"2025-10-17T17:00:05.043709Z","shell.execute_reply":"2025-10-17T17:00:05.078898Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'vit_model_graph.pdf'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from torchviz import make_dot\nimport torch\n\nx = torch.randn(1, 3, 518, 518).to(device)\nout = model.head(model.norm(model.forward_features(x)))  # skip details\n\n# Create a simpler graph with only named parameters\ndot = make_dot(out, params={k: v for k, v in model.named_parameters() if \"blocks\" not in k})\ndot.render(\"vit_highlevel\", format=\"pdf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:59:10.655240Z","iopub.execute_input":"2025-10-17T16:59:10.655553Z","iopub.status.idle":"2025-10-17T16:59:11.693434Z","shell.execute_reply.started":"2025-10-17T16:59:10.655529Z","shell.execute_reply":"2025-10-17T16:59:11.692526Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'vit_highlevel.pdf'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from torchviz import make_dot\n\nx = torch.randn(1, 3, 518, 518).to(device)\nout = model.forward_head(model.forward_features(x), pre_logits=True)\ndot = make_dot(out)\ndot.render(\"vit_toplevel\", format=\"pdf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:59:22.325869Z","iopub.execute_input":"2025-10-17T16:59:22.326195Z","iopub.status.idle":"2025-10-17T16:59:22.664679Z","shell.execute_reply.started":"2025-10-17T16:59:22.326166Z","shell.execute_reply":"2025-10-17T16:59:22.663919Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'vit_toplevel.pdf'"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"Input Image\n   ↓\nPatch Embedding (Conv2D)\n   ↓\nPositional Encoding\n   ↓\n[×12 Transformer Blocks]\n   ↓\nLayerNorm\n   ↓\nLinear Head → Predictions\n","metadata":{}},{"cell_type":"markdown","source":"### Model Overview: ViT Base (DINOv2)\n\nInput: Image of size 518×518×3 (height × width × channels).\n\nPatch Embedding:\n\nSplits the image into 14×14 patches → each patch projected into 768-dimensional embedding.\n\nPositional Encoding & Dropout:\n\nAdds positional information to patches; dropout is 0 here.\n\nTransformer Encoder:\n\n12 Transformer blocks (blocks)\n\nEach block has:\n\nLayerNorm → Multi-Head Self-Attention → MLP → LayerNorm\n\nAttention: Queries, Keys, Values → 768-dim → 768-dim\n\nMLP: 768 → 3072 → 768 with GELU activation\n\nResidual connections + optional LayerScale\n\nFinal Normalization:\n\nLayerNorm applied to the final feature vector.\n\nClassification Head (MLP):\n\nLinear(768 → 7806) → produces logits for 7806 classes (your dataset).\n\nOutput: Probability vector over all 7806 classes (after softmax during inference).","metadata":{}},{"cell_type":"code","source":"print(model.head)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:00:12.775788Z","iopub.execute_input":"2025-10-17T17:00:12.776078Z","iopub.status.idle":"2025-10-17T17:00:12.780783Z","shell.execute_reply.started":"2025-10-17T17:00:12.776057Z","shell.execute_reply":"2025-10-17T17:00:12.779875Z"}},"outputs":[{"name":"stdout","text":"Linear(in_features=768, out_features=7806, bias=True)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"The 768 extracted features are used to predict among 7806 possible classes.\n\n* The Vision Transformer (ViT) backbone extracts a 768-dimensional feature vector that represents the image’s content.\n\n* The final linear (fully connected) layer acts as the classifier head, which maps those 768 features into 7806 output neurons.\n\n* Each output neuron corresponds to one class (species) in your dataset — since you have 7806 unique species IDs.\n\n* The layer learns how to combine the 768 features to produce a score (logit) for each class.\n\n* During inference, a softmax converts those 7806 logits into probabilities, and the class with the highest probability becomes the prediction.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}